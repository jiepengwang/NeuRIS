<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0051)https://lingjie0206.github.io/papers/NeuS/index.htm -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>NeuRIS: Neural Reconstruction of Indoor Scenes Using Normal Priors </title>
	


	<!-- Meta tags for Zotero grab citation -->
	<meta name="citation_title" content="NeuRIS: Neural Reconstruction of Indoor Scenes Using Normal Priors">
	<meta name="citation_author" content="Wang, Jiepeng">
	<meta name="citation_author" content="Wang, Peng">
	<meta name="citation_author" content="Long, Xiaoxiao">
	<meta name="citation_author" content="Theobalt, Christian">
	<meta name="citation_author" content="Komura, Taku">
	<meta name="citation_author" content="Liu, Lingjie">
	<meta name="citation_author" content="Wang, Wenping">
	<meta name="citation_publication_date" content="2022">
	<meta name="citation_conference_title" content="arxiv">
	<meta name="citation_pdf_url" content="https:">

	<meta name="robots" content="index,follow">
	<meta name="description" content="
			Reconstructing 3D indoor scenes from 2D images is an important task in many computer vision and graphics applications. A main challenge in this task is that large texture-less areas in typical indoor scenes make existing methods struggle to produce satisfactory reconstruction results. We propose a new method, dubbed NeuRIS, for high quality reconstruction of indoor scenes. The key idea of NeuRIS is to integrate estimated normal vectors of indoor scenes as a prior in a neural rendering framework for reconstructing large texture-less shapes and, importantly, does so in an adaptive manner to also enable the reconstruction of irregular shapes with fine details. Specifically, we evaluate the faithfulness of the normal priors on-the-fly by checking the multi-view consistency of reconstruction during the optimization process. Only the normal priors accepted as faithful will be utilized for 3D reconstruction, which typically happens in regions of smooth shapes possibly with weak texture. However, for those regions with small objects or thin structures, for which the normal priors are usually unreliable, we will only rely on visual features of the input images, since such regions typically contain relatively rich visual features (e.g., shade changes and boundary contours). Extensive experiments show that NeuRIS significantly outperforms the state-of-the-art methods in terms of reconstruction quality.				<p></p>
		">
	<link rel="author" href="https://lingjie0206.github.io/">


	<!-- Fonts and stuff -->
	<link href="./images/css" rel="stylesheet" type="text/css">
	<link rel="stylesheet" type="text/css" href="./images/project.css" media="screen">
	<link rel="stylesheet" type="text/css" media="screen" href="./images/iconize.css">
	<script src="./images/prettify.js.download"></script>
</head>

<body>
	<div id="content">
		<div id="content-inner">
			<div class="section logos" style="text-align:center">
				<a href="https://www.hku.hk/" target="_blank"><img src="./images/hku_logo.jpg" height="35" border="0"></a>
				
				<a href="http://www.mpi-inf.mpg.de/home/" target="_blank"><img src="./images/Logo_MPII.png" height="35" border="0"></a>
				<a href="https://lingjie0206.github.io/index.html" target="_blank"><img src="./images/Logo_gvv.png" height="35" border="0"></a>
				
				<a href="https://www.tamu.edu/" target="_blank"><img src="./images/Texas-AM-University_logo.jpg" height="35" ="0"=""></a>
			</div>

			<div class="section head">

				<h1>NeuRIS: Neural Reconstruction of Indoor Scenes Using Normal Priors</h1>

				<div class="authors">
					<a href="https://jiepengwang.github.io" target="_blank">Jiepeng Wang</a><sup> 1</sup>&nbsp;&nbsp;
					<a href="https://totoro97.github.io/about.html" target="_blank">Peng Wang</a><sup> 1</sup>&nbsp;&nbsp;
					<a href="https://www.xxlong.site" target="_blank">Xiaoxiao Long</a><sup> 1</sup>&nbsp;&nbsp;
					
					<a href="http://people.mpi-inf.mpg.de/~theobalt/" target="_blank">Christian Theobalt</a><sup>
						2</sup>&nbsp;&nbsp;
					<a href="https://www.cs.hku.hk/index.php/people/academic-staff/taku">Taku Komura</a><sup> 1</sup>&nbsp;&nbsp;
					<a href="https://lingjie0206.github.io/" target="_blank">Lingjie Liu</a><sup> 2</sup>&nbsp;&nbsp;
					<a href="https://www.cs.hku.hk/people/academic-staff/wenping">Wenping Wang</a><sup> 3</sup>&nbsp;&nbsp;
				</div>

				<div class="affiliations">
					<sup>1</sup><a href="https://www.hku.hk/" target="_blank">University of Hong Kong</a>&nbsp;&nbsp;
					<sup>2</sup><a href="http://www.mpi-inf.mpg.de/home/" target="_blank">MPI Informatics, Saarland
						Informatics Campus</a>&nbsp;&nbsp;
					<sup>3</sup><a href="https://www.tamu.edu/" target="_blank">Texas A&amp;M University</a>&nbsp;&nbsp;
				</div>
				<!--<div class="venue">NeurIPS 2021 (Spotlight) </div>-->

				<div class="section downloads">
					<!--<h2>Downloads</h2>-->
					<center>
						<ul>
							<li class="grid">
								<div class="griditem">
									<a href="https://arxiv.org/abs/2206.13597" target="_blank" class="imageLink"><img src="./images/pdf.png"></a><br>
									<a href="https://arxiv.org/abs/2206.13597">Paper</a>
								</div>
							</li>
							<li class="grid">
								<div class="griditem">
									<a href="https://drive.google.com/file/d/1qmP2VZ8rf1mOgjhb1KWwnxqLTDhjmlGA/view?usp=sharing" target="_blank" class="imageLink"><img src="./images/pdf.png"></a><br>
									<a href="https://drive.google.com/file/d/1qmP2VZ8rf1mOgjhb1KWwnxqLTDhjmlGA/view?usp=sharing">Supp</a>

								</div>
							</li>
							<li class="grid">
								<div class="griditem">
									<a href="https://github.com/jiepengwang/NeuRIS" target="_blank" class="imageLink"></a><img src="./images/data_ico.png"><br>
									<a href="https://github.com/jiepengwang/NeuRIS">Code</a>
								</div>
							</li>
						</ul>
					</center>
				</div>
			</div>




			<div class="section abstract">
				<h2>Abstract</h2><br>
				<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="./images/teaser.png" style="width:80%; margin-bottom:20px">

					</div>

				</div>

				<p class="text-justify">
					Reconstructing 3D indoor scenes from 2D images is an important task in many computer vision and graphics applications. A main challenge in this task is that large texture-less areas in typical indoor scenes make existing methods struggle to produce satisfactory reconstruction results. We propose a new method, dubbed NeuRIS, for high quality reconstruction of indoor scenes. The key idea of NeuRIS is to integrate estimated normal vectors of indoor scenes as a prior in a neural rendering framework for reconstructing large texture-less shapes and, importantly, does so in an adaptive manner to also enable the reconstruction of irregular shapes with fine details. Specifically, we evaluate the faithfulness of the normal priors on-the-fly by checking the multi-view consistency of reconstruction during the optimization process. Only the normal priors accepted as faithful will be utilized for 3D reconstruction, which typically happens in regions of smooth shapes possibly with weak texture. However, for those regions with small objects or thin structures, for which the normal priors are usually unreliable, we will only rely on visual features of the input images, since such regions typically contain relatively rich visual features (e.g., shade changes and boundary contours). Extensive experiments show that NeuRIS significantly outperforms the state-of-the-art methods in terms of reconstruction quality.				<p></p>
				</p>

			</div>

			<!--<div class="section abstract">
				<h2>Full Video</h2><br>
				<center>
					<!-- <iframe width="640" height="360" src="data/video.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
					<!--<iframe width="640" height="360" src="https://www.youtube.com/embed/RFqPwH7QFEI" frameborder="0"
						allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
						allowfullscreen></iframe>-->
					<!--iframe src="./data/video.mp4" allow="autoplay; encrypted-media" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" width="560" height="315" frameborder="0"></iframe-->
					<!--<p style="font-size:11px; text-align:center">
					Download Video: <a href="data/video.mp4" target="_blank">HD</a> (MP4, 111 MB)
				</p>
				</center>
			</div>-->

			<div class="section abstract">
				<h2>Introduction</h2>
				
				<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="./images/pipeline.png" style="width:80%; margin-bottom:20px">
					</div>
				</div>
				
				<p class="text-justify">
					Our method is composed by two phases. In the first phase, we train a coarse model to fit both the multi-view images and the estimated normal maps by volume rendering, without any filtering strategy. In the second phase, we adaptively impose the supervision from normal priors, where two branches are performed simultaneously: in one branch we conduct a geometric quality evaluation by computing multi-view visual consistency; in the other branch, only those prior normals that pass the geometric check are accepted as proper supervisions to the rendered normals.				</p>
				</p>
			</div>

			<div class="section abstract">
				<h2>Comparisons on Scannet</h2><br>
				
				<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<video width="80%" playsinline="" controls autoplay loop="loop" preload="" muted="">
						<source src="data/bath_room.mp4" type="video/mp4">
					</video>
					</div>

				</div>
			</div>
			
			<div class="section abstract">
				<h2>Results on our data (by iPhone 11)</h2><br>
				<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<!--<video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">-->
						<video width="80%" playsinline="" controls autoplay loop="loop" preload="" muted="">
						<source src="data/living_room.mp4" type="video/mp4">
					</video>
					<!--<h3>Scan 37 from the DTU dataset</h3><br>-->

					</div>

				</div>
			</div>
			

			<div class="section abstract">
				<h2>Results on Hypersim </h2><br>
				<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<img class="thumbnail" src="./images/hypersim.png" style="width:80%; margin-bottom:20px">

					</div>
				</div>
				<div class="row" style="margin-bottom:5px">
					<div class="col" style="text-align:center">
						<video width="80%" playsinline="" controls autoplay loop="loop" preload="" muted="">
						<source src="data/hypersim.mp4" type="video/mp4">
					</video>
					<!--<h3>Scenes from the BlendedMVS dataset</h3><br>-->
				</div>
			</div>		
			

	
			<div class="section list">
				<h2>Citation</h2>
				<div class="section bibtex">
					<pre>@article{wang2022neuris,
      	title={NeuRIS: Neural Reconstruction of Indoor Scenes Using Normal Priors}, 
      	author={Wang, Jiepeng and Wang, Peng and Long, Xiaoxiao and Theobalt, Christian and Komura, Taku and Liu, Lingjie and Wang, Wenping},
	journal={arXiv preprint},
      	year={2022}
}
				</pre></div>
			</div>
			
			<!--<div class="section list">
				<h2>Related Links</h2>
				<div class="row" style="margin-top:15px">
				<li>Parts of <a href="https://github.com/Totoro97/NeuS" target="_blank">our PyTorch implementation</a> are taken from <a href="https://github.com/lioryariv/idr" target="_blank">IDR</a> and <a href="https://github.com/yenchenlin/nerf-pytorch" target="_blank">NeRF-pytorch</a>.
				</li><li>Check the concurrent works of learning neural implicit surfaces: <br> 			
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://arxiv.org/abs/2104.10078" target="_blank">UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction</a>, Oechsle et al. 2021 <br>
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://arxiv.org/abs/2106.12052" target="_blank">Volume Rendering of Neural Implicit Surfaces</a>, Yariv et al. 2021 
				</li><li>Also check other works about neural scene representations and neural rendering from our group: <br> 
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://lingjie0206.github.io/papers/NSVF/" target="_blank">Neural Sparse Voxel Fields:</a>, Liu et al. 2020 <br> 	
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/" target="_blank">Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video</a>, Tretschk et al. 2021<br> 	
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://gvv.mpi-inf.mpg.de/projects/NeuralActor/" target="_blank">Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control</a>, Liu et al. 2021 <br> 	
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://liuyuan-pal.github.io/NeuRay/" target="_blank">Neural Rays for Occlusion-aware Image-based Rendering</a>, Liu et al. 2021. <br> 	
				</li></div>
			</div>
			
			<div class="section list">
				<h2>Acknowledgements</h2>
				<div class="row" style="margin-top:15px">
				<p>We thank Michael Oechsle for providing the results of UNISURF. Christian Theobalt was supported by ERC Consolidator Grant 770784. Lingjie Liu was supported by Lise Meitner Postdoctoral Fellowship.</p> 
				</div>
			</div>
			-->
			
			
			<div class="section">
				<hr class="smooth">
				This page is <a href="http://www.zotero.org/" target="_blank">Zotero</a> translator friendly. Page last updated 
				<script type="text/javascript">
					var m = "This page was last updated: " + document.lastModified;
					var p = m.length - 9;
					document.writeln("<left>");
					document.write(m.substring(p, 0) + ".");
					document.writeln("</left>");
				</script><left>
This page was last updated: 05/09/2022.</left>

				<a href="https://www.mpi-inf.mpg.de/imprint/">Imprint</a>. <a href="https://data-protection.mpi-klsb.mpg.de/inf/gvv.mpi-inf.mpg.de/projects/">Data Protection</a>.
			</div>
		</div>
	</div>

</div></body></html>
